Divide and Conquer:
    - Break into non-overlaping subproblems that are themselfs smaller instance
    of same type of problem - Recursively solve the subproblems - Appropriately
    Combine results

Recurrence relation:
    A Recurrence relation is an equation recursively defining a sequence of
    value. Divide-and-conquer algorithms often follow a generic pattern: they
    tackle a problem of size n by recursively solving, say,a subproblems of size
    n/b and then combining these answers in O(n^d) time, for some a;b;d > 0.
    Their running time can therefore be captured by the equation T(n) = a
    T(celling(n/b)) + O(n^d).

Master theorm:
    Provides recurrance relation to find runtime complexity of most of the
    divide-and-conquer procedures:
        If T(n) = a T(celling(n/b)) + O(n^d) for some constant a>0; b>1; d>=0
        Then,
            T(n) = O(n^d) if d > log<b>(a)
                 = O(n^d log(n)) if d = log<b>(a) 
                 = O(n^(log<b>(a))) if d <log<b>(a)

Linear search:
    Sequential search made over all item one-by Recurrence relation defining
    worst case runtime: 
        T(n) = T(n-1) + c
    Runtime complexity: O(n)

Binary search: 
    Works if data is in sorted form. Binary search looks for a particular item by
    comparing the middle most item of the collection. If a match occurs, then
    the index of item is returned. If the middle item is greater than the item,
    then the item is searched in the sub-array to the left of the middle item.
    Otherwise, the item is searched for in the sub-array to the right of the
    middle item. This process continues on the sub-array as well until the size
    of the subarray reduces to zero. Runtime compexity: O(log<2>n)
        If we are cutting somthing of size n into half over and over again then
        it takes log<2>n times before we get down to 1

Polynomial multipication:
    Naive: 
        Idea: 
            For two polynomials of order n-1, say, a(x) = a<1>x^(n-1) +
            a<2>x^(n-2) + ... + a<0> and b(x) b<1>x^(n-1) + b<2>x^(n-2) + ... +
            b<0> The product polynomial will be of order 2n-1, say, c(x) =
            c<2n-2>x^(2n-2) + c<2n -1>x^(2n-1)+....+c<0> where:
                c<2n-2> = a<n-1>b<n-1> c<2n-3> = a<n-1>b<n-2> + a<n-2>b<n-1> ...
                c<2> = a<2>b<0> + a<1>b<1> + a<0>b<2> c<1> = a<1>b<0> + a<0>b<1>
                c<0> = a<0>b<0>
        Use above idea Using two iterative loop to calculate all the
        coefficients. Notice that coeffiecent c<k> is sum of all those a<i>b<j>
        such that i+j = k. Runtime complexity: O(n^(2))
    Naive divide and conquer: 
        Idea: 
            Consider two polnomials, a(x) = a<1>x^(n-1) + a<2>x^(n-2) + ... +
            a<0> and b(x) b<1>x^(n-1) + b<2>x^(n-2) + ... + b<0> We could write,
            a(x) = d<1>(x)x^(n/2) + d<0>(x) b(x) = e<1>(x)x^(n/2) + e<0>(x) then
            a(x)b(x) = (d<1>e<1>)x^(n) + (d<1>e<0> + d<0>e<1>)x^(n/2) + d<0>e<0>
            Note: we could always pad the polynomial in a way that n is multiple
            of 2
        Divide problem os size n into 4 subproblem of size n/2 using above idea
        and keep doing it recursively till its become subproblems of size 1.
        Hence, we could write recursive realtion as:
            T(n) = 4T(n/2) + O(n) where O(n) is time required to evaluate
            expression (apart from solving subproblems)
        Runtime complexity: O(n^(log<2>4)) or O(n^2)
    Fast divide and conquer:
        Idea: 
            x = ac + ad + bc + bd involves 4 multiplications whereas same could
            be done with just 3 multiplication because 
                bc + bd = (a+b)(c+d) - ac - ad
        Divide problem into 3 sub-problem at each level using above concept
        Hence, we could write recursive realtion as:
            T(n) = 3T(n/2) + O(n) where O(n) is time required to evaluate
            expression (apart from solving subproblems)
        Runtime complexity: O(n^(log<2>3)) or O(n^(1.58)) which significant
        reduction in runtime complexcity in compare to naive divide-and-conquer
        because of recursive application of small trick mentioned above
    Note: Idea of polynomial multipication could be easily applied to
    multiplication of two n digit numbers. 
        For two n digit numbers a = a<1>a<2>...a<n> and b = b<1>b<2>...b<n>
        create two polynomials a(x) = a<1>x^(n-1) + a<2>x^(n-2) + ... + a<n> and
        b(x) b<1>x^(n-1) + b<2>x^(n-2) + ... + b<n> respectively. Next, do
        polynomial multiplication c(x) = a(x)b(x). For each coefficient of c
        which is greated than 9, keep the digit at unit place and add 1 to the
        coefficient left to it. Now replace x in c(x) with 10 to get product of
        two digits

Sorting:
    Selection sort:
        This sorting algorithm is an in-place comparison-based algorithm in
        which the list is divided into two parts, the sorted part at the left
        end and the unsorted part at the right end. Initially, the sorted part
        is empty and the unsorted part is the entire list.The smallest element is selected from the unsorted array and swapped
        with the leftmost element, and that element becomes a part of the sorted
        array. This process continues moving unsorted array boundary by one
        element to the right.
        runtime-complexity: O(n^2)

    Merge sort:
        It uses divide-and-conquer algorithm startegy to solve sorting problem.
        It recursively divides the array into equal halves, sort them and then
        combines them in a sorted manner till subproblem size is one. For 
        combinig, it simply checks first element of the sorted array and move the 
        one which is smaller to a the array storing combined array till both 
        sorted array reduce to size 0.
        Runtime complexity:
            Recursive relation: 
                T(n) = 2T(n/2) + O(n)
                where 0(n) is time combine the solved subproblems
            Using master theorm runtime complexity of merge sort could be given 
            by O(n(log(n)))
            Any comparison based sorting algorithm will have runtime complexity 
            of >= O(nlog(n)) in worst case which means merge sort is 
            asymptotically optimal.
    
    Counting sort:
        Sorting without comparison
        if we know the range of element in integer array then scan through array
         and count the occurence of each element then create sorted array based 
         on the count.
        Runtime complexity: O(n+m) 
        where n is number of elelment in array and m is size of the range of the 
        element.

    Randomized quick sort:
        Comparison based algorithm 
        Strategy:
            Generate an index between lower and upper bound of array and swap 
            first element of array with the element at generated random index
            Take first element of of array A, call it pivot x = A[l]
            Then move i on l+1 to r (the end of the array) maintaining following 
            invariant:
                A[k] <= x for all l+1 <= k <=j<1>
                A[k] = x for all j<1>+1 <= k <= j<2> 
                A[k] >= x for all j<2>+1 <= k <= r
                you can do this by starting with j = l and as you increment i,
                incrementing j by 1 followed by swapping A[j] with A[i] when you 
                encounter A[i] > x.  
            At the end move A[l] to its final place by swapping A[l] and A[j]
            Do it recursively to sort the complete array
        Runtime complexity: 
            O(nlog(n)) (on average, while worst case runtime is o(n^2))
        Space complexity:
            In general: O(n)
            Could be reduced to: O(log(n))


Reading resources:
     Sanjoy Dasgupta, Christos Papadimitriou, and Umesh Vazirani. Algorithms
     (1st Edition). McGraw-Hill Higher Education. 2008.
        Polynomial multiplication: Section 2.1
        Master Theorem: Section 2.2
        Merge sort and lower bound for comparison based sorting: Section 2.3
    Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein.
    Introduction to Algorithms (3rd Edition). MIT Press and McGraw-Hill. 2009.
        Quick sort: Chapter 7
